# GPT-2 Inference Engine — From Scratch

A multi-backend GPT-2 inference engine built from the ground up. The same model weights power 5 different implementations, from high-level Python to SIMD-optimized C++ with KV caching.

## Backends

| Backend | Language | Description | Speed |
|---------|----------|-------------|-------|
| **hf** | Python | HuggingFace `transformers` library | ~27 tok/s |
| **torch_native** | Python | Hand-written PyTorch (no library inference code) | ~32 tok/s |
| **numpy** | Python | Pure NumPy — no PyTorch at runtime | ~5 tok/s |
| **cpp_simd** | C++ | Apple Accelerate BLAS + ARM NEON intrinsics | ~50 tok/s |
| **cpp_kv** | C++ | SIMD + KV Cache (only computes 1 new token/step) | **~124 tok/s** |

*Benchmarked on Apple Silicon with `--max_length 100`.*

## Quick Start

```bash
# 1. Setup
python -m venv .venv && source .venv/bin/activate
pip install torch transformers numpy

# 2. Export weights (one-time, creates weights/model.bin + weights/vocab.bin)
python export_weights.py

# 3. Build C++ backends (macOS only, requires Xcode CLI tools)
cd cpp && make && cd ..

# 4. Run single backend
python main.py --backend hf --prompt "Hello world" --max_length 50

# 5. Benchmark all backends
python main.py --compare --prompt "The meaning of life is" --max_length 50
```

## Project Structure

```
├── main.py                  # Entry point: generation loop + benchmark
├── export_weights.py        # Export GPT-2 → weights/model.bin + vocab.bin
├── core/
│   ├── interfaces.py        # TokenizerInterface, TransformerEngineInterface
│   ├── sampler.py           # Temperature sampling / greedy decoding
│   └── weight_loader.py     # Shared binary weight loader (model.bin → numpy)
├── backends/
│   ├── hf_backend.py        # HuggingFace GPT2LMHeadModel wrapper
│   ├── torch_native_backend.py  # Hand-coded PyTorch GPT-2
│   └── numpy_backend.py     # Pure NumPy GPT-2
├── cpp/
│   ├── main.cpp             # C++ entry point (shared by all C++ variants)
│   ├── model.h              # Naive C++ (triple-loop matmul)
│   ├── model_simd.h         # SIMD: cblas_sgemm + ARM NEON + vvexpf/vvtanhf
│   ├── model_kvcache.h      # SIMD + KV Cache
│   ├── encode_text.py       # Helper: tokenize text for C++ input
│   └── Makefile             # Builds: gpt2, gpt2_fast, gpt2_kv
└── weights/                 # Generated by export_weights.py
    ├── model.bin            # All GPT-2 weights (475 MB)
    └── vocab.bin            # BPE vocabulary (50,257 tokens)
```

## Architecture

All backends share the same weight file (`weights/model.bin`) and produce **identical output** under greedy decoding.

```
export_weights.py
    │
    ▼
weights/model.bin ──────┬──► hf_backend.py ──────► GPT2LMHeadModel
(shared binary format)  ├──► torch_native.py ────► ManualGPT2 (PyTorch)
                        ├──► numpy_backend.py ───► Pure NumPy forward pass
                        ├──► cpp/gpt2_fast ──────► SIMD C++ (Accelerate + NEON)
                        └──► cpp/gpt2_kv ────────► SIMD + KV Cache
```

## Weight Format (`model.bin`)

Binary file, all `float32`, no compression:

```
Header:  [n_layer, n_head, n_embd, vocab_size, max_seq_len]  (5 × int32)
Body:    wte, wpe, then per-layer weights (LN, attention, MLP), then ln_f
```

## C++ Backends

The C++ backends run standalone — no Python required at runtime.

```bash
# Tokenize input text
python cpp/encode_text.py "Hello world"
# Output: 15496 995

# Run inference
./cpp/gpt2_kv --ids "15496 995" --max_length 50 \
    --model weights/model.bin --vocab weights/vocab.bin
```

## Key Optimizations

| Optimization | Where | Impact |
|---|---|---|
| **cblas_sgemm** | `model_simd.h` | ~23× faster matmul vs naive triple-loop |
| **vvexpf / vvtanhf** | `model_simd.h` | Vectorized transcendental functions |
| **ARM NEON** | `model_simd.h` | 4-wide SIMD for element-wise ops |
| **KV Cache** | `model_kvcache.h` | O(C²) per step instead of O(T·C²) |

## License

Educational project. GPT-2 weights are from OpenAI via HuggingFace.
